{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05117041",
   "metadata": {},
   "source": [
    "# Object Storage and Data Pipeline Setup\n",
    "\n",
    "This notebook demonstrates how to set up and manage data pipelines using rclone and object storage on Chameleon Cloud.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- GPU instance already created on Chameleon Cloud\n",
    "- rclone is pre-installed with S3 credentials configured\n",
    "- Access to Chameleon Cloud GUI for bucket creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17bbe2",
   "metadata": {},
   "source": [
    "## Step 1: Initial Setup\n",
    "\n",
    "### Create S3 Bucket\n",
    "1. Navigate to the Chameleon Cloud GUI\n",
    "2. Create a new bucket in your desired region\n",
    "3. Name the bucket appropriately for your project\n",
    "\n",
    "### Configure FUSE on GPU Instance\n",
    "SSH into your GPU instance and run the following command to enable user access to FUSE:\n",
    "\n",
    "```bash\n",
    "sudo sed -i '/^#user_allow_other/s/^#//' /etc/fuse.conf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1706e3",
   "metadata": {},
   "source": [
    "### Verify rclone Configuration\n",
    "Check if S3 configuration is already present in the rclone configuration file:\n",
    "\n",
    "```bash\n",
    "nano ~/.config/rclone/rclone.conf\n",
    "```\n",
    "\n",
    "Look for existing `rclone_s3` configuration in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2309706",
   "metadata": {},
   "source": [
    "## Step 2: Data Pipeline Setup\n",
    "\n",
    "### Overview\n",
    "We'll create a pipeline to load training data using the existing `rclone_s3` configuration in the rclone.conf file. This pipeline will transfer data from the object store to the GPU instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fe83c",
   "metadata": {},
   "source": [
    "### ETL Pipeline Execution\n",
    "\n",
    "The Docker Compose configuration (`docker-compose-etl-s3.yaml`) contains all the necessary pipelines to load data from the object store to the GPU instance.\n",
    "\n",
    "#### 1. Extract Data\n",
    "```bash\n",
    "docker compose -f docker-compose-etl-s3.yaml run extract-data\n",
    "```\n",
    "\n",
    "#### 2. Transform Data\n",
    "```bash\n",
    "docker compose -f docker-compose-etl-s3.yaml run transform-data\n",
    "```\n",
    "\n",
    "#### 3. Load Data\n",
    "First, set your container name environment variable:\n",
    "```bash\n",
    "export RCLONE_CONTAINER=object-persist-YOURNETID\n",
    "```\n",
    "\n",
    "Then run the load data pipeline:\n",
    "```bash\n",
    "docker compose -f docker-compose-etl-s3.yaml run load-data\n",
    "```\n",
    "\n",
    "For sharded data to be loaded, use:\n",
    "```bash\n",
    "docker compose -f docker-compose-etl-s3.yaml run shard-data\n",
    "```\n",
    "\n",
    "> **Note**: Replace `YOURNETID` with your actual NetID in the container name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f34ca",
   "metadata": {},
   "source": [
    "## Step 3: Cleanup and Troubleshooting\n",
    "\n",
    "If something goes wrong and you need to clean everything up and start from scratch, follow these steps in order:\n",
    "\n",
    "### 1. Stop All Containers Using food11 Volume\n",
    "```bash\n",
    "docker ps -a --filter volume=food11-etl-s3_food11 -q | xargs -r docker rm -f\n",
    "```\n",
    "\n",
    "### 2. Bring Down Compose and Remove Volumes\n",
    "```bash\n",
    "docker compose -f docker-compose-etl-s3.yaml down -v\n",
    "```\n",
    "\n",
    "### 3. Remove Any Remaining food11 Volumes\n",
    "```bash\n",
    "docker volume ls | grep food11 | awk '{print $2}' | xargs -r docker volume rm\n",
    "```\n",
    "\n",
    "### 4. Clean Up Orphan Containers\n",
    "```bash\n",
    "docker container prune -f\n",
    "```\n",
    "\n",
    "### 5. Delete Data from Object Store (Optional)\n",
    "```bash\n",
    "rclone delete rclone_s3:YOUR_CONTAINER_NAME --rmdirs\n",
    "```\n",
    "\n",
    "> **Warning**: Step 5 will permanently delete data from your object store. Only run this if you're certain you want to remove all data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55cd8e",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [rclone Documentation](https://rclone.org/docs/)\n",
    "- [Chameleon Cloud Documentation](https://chameleoncloud.readthedocs.io/)\n",
    "- [Docker Compose Documentation](https://docs.docker.com/compose/)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After completing this pipeline setup, you can:\n",
    "1. Monitor your data transfer progress\n",
    "2. Verify data integrity after transfer\n",
    "3. Set up automated data synchronization schedules\n",
    "4. Configure additional object storage backends if needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
