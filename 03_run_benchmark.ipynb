{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ab0bcc",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b34f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11551875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145f28a",
   "metadata": {},
   "source": [
    "## Part 2: Configuration\n",
    "\n",
    "**Edit this cell to change rclone mount options before each run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28175cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_CSV = 'training_benchmark_results.csv'\n",
    "GPU_CSV = 'gpu_utilization.csv'\n",
    "\n",
    "# Unique identifier for this test run\n",
    "RUN_ID = \"test_001\"\n",
    "\n",
    "# rclone remote and container\n",
    "RCLONE_REMOTE = \"rclone_s3\"  # or the swift configuration in the rclone config\n",
    "RCLONE_CONTAINER = \"object-persist-YOURNETID\"  # Your container name\n",
    "\n",
    "# Mount point\n",
    "MOUNT_POINT = \"/mnt/object\"\n",
    "\n",
    "\n",
    "RCLONE_OPTIONS = {\n",
    "    # Cache settings\n",
    "    'vfs_cache_mode': 'full',        # off, minimal, writes, full\n",
    "    'vfs_cache_max_size': '20G',     # e.g., 5G, 10G, 20G, 50G\n",
    "    'vfs_cache_max_age': '1h',       # e.g., 1h, 24h\n",
    "    \n",
    "    # Read performance\n",
    "    'vfs_read_chunk_size': '64M',    # e.g., 16M, 64M, 128M, 256M\n",
    "    'vfs_read_chunk_size_limit': '512M',  # e.g., 256M, 512M, off (unlimited)\n",
    "    'vfs_read_ahead': '256M',        # e.g., 128M, 256M, 512M, 1G\n",
    "    'buffer_size': '128M',           # e.g., 16M, 64M, 128M, 256M\n",
    "    \n",
    "    # Parallelism\n",
    "    'transfers': '16',               # e.g., 4, 8, 16, 32\n",
    "    'checkers': '8',                 # e.g., 4, 8, 16\n",
    "    \n",
    "    # Directory caching\n",
    "    'dir_cache_time': '30m',         # e.g., 5m, 30m, 1h\n",
    "    'attr_timeout': '30s',           # e.g., 1s, 10s, 30s, 1m\n",
    "    \n",
    "    # Stability/Retry settings\n",
    "    'low_level_retries': '10',\n",
    "    'retries': '3',\n",
    "    'contimeout': '30s',\n",
    "    'timeout': '120s',\n",
    "}\n",
    "\n",
    "\n",
    "DATALOADER_OPTIONS = {\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 8,\n",
    "    'prefetch_factor': 4,\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Run ID: {RUN_ID}\")\n",
    "print(f\"  Remote: {RCLONE_REMOTE}:{RCLONE_CONTAINER}\")\n",
    "print(f\"  Cache Mode: {RCLONE_OPTIONS['vfs_cache_mode']}\")\n",
    "print(f\"  Batch Size: {DATALOADER_OPTIONS['batch_size']}\")\n",
    "print(f\"  Num Workers: {DATALOADER_OPTIONS['num_workers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081b884",
   "metadata": {},
   "source": [
    "## Part 3: Mount Object Store\n",
    "\n",
    "Run this cell to mount the object store with the configured options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mount_command(remote, container, mount_point, options):\n",
    "    \"\"\"Build rclone mount command from options dictionary.\"\"\"\n",
    "    cmd = f\"rclone mount {remote}:{container} {mount_point}\"\n",
    "    \n",
    "    # Add options\n",
    "    cmd += \" --read-only --allow-other\"\n",
    "    \n",
    "    option_map = {\n",
    "        'vfs_cache_mode': '--vfs-cache-mode',\n",
    "        'vfs_cache_max_size': '--vfs-cache-max-size',\n",
    "        'vfs_cache_max_age': '--vfs-cache-max-age',\n",
    "        'vfs_read_chunk_size': '--vfs-read-chunk-size',\n",
    "        'vfs_read_chunk_size_limit': '--vfs-read-chunk-size-limit',\n",
    "        'vfs_read_ahead': '--vfs-read-ahead',\n",
    "        'buffer_size': '--buffer-size',\n",
    "        'transfers': '--transfers',\n",
    "        'checkers': '--checkers',\n",
    "        'dir_cache_time': '--dir-cache-time',\n",
    "        'attr_timeout': '--attr-timeout',\n",
    "        'low_level_retries': '--low-level-retries',\n",
    "        'retries': '--retries',\n",
    "        'contimeout': '--contimeout',\n",
    "        'timeout': '--timeout',\n",
    "    }\n",
    "    \n",
    "    for key, flag in option_map.items():\n",
    "        if key in options and options[key]:\n",
    "            cmd += f\" {flag} {options[key]}\"\n",
    "    \n",
    "    cmd += \" --daemon\"\n",
    "    return cmd\n",
    "\n",
    "# Build the mount command\n",
    "mount_cmd = build_mount_command(RCLONE_REMOTE, RCLONE_CONTAINER, MOUNT_POINT, RCLONE_OPTIONS)\n",
    "print(\"Mount command:\")\n",
    "print(mount_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3856abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmount if already mounted\n",
    "print(\"Checking for existing mount...\")\n",
    "unmount_result = subprocess.run(f\"fusermount -u {MOUNT_POINT}\", shell=True, capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "if unmount_result.returncode == 0:\n",
    "    print(f\"Unmounted existing mount at {MOUNT_POINT}\")\n",
    "else:\n",
    "    print(f\"No existing mount at {MOUNT_POINT} or unmount failed, proceeding...\")\n",
    "\n",
    "# Mount with new configuration\n",
    "print(f\"Mounting {RCLONE_REMOTE}:{RCLONE_CONTAINER} to {MOUNT_POINT}...\")\n",
    "result = subprocess.run(mount_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"ERROR: Mount failed!\")\n",
    "    print(f\"stderr: {result.stderr}\")\n",
    "else:\n",
    "    # Wait for mount to be ready\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Verify mount\n",
    "    if os.path.exists(MOUNT_POINT) and os.listdir(MOUNT_POINT):\n",
    "        print(f\"SUCCESS: Mounted successfully!\")\n",
    "        print(f\"Contents: {os.listdir(MOUNT_POINT)}\")\n",
    "    else:\n",
    "        print(\"WARNING: Mount point exists but appears empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1de80",
   "metadata": {},
   "source": [
    "## Part 4: Run Training Benchmark\n",
    "\n",
    "This section runs model training with proper evaluation and test, measuring both data loading performance and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = MOUNT_POINT\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Contents: {os.listdir(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import gc\n",
    "\n",
    "def run_training_benchmark(data_dir, batch_size, num_workers, epochs=3, \n",
    "                          prefetch_factor=2, pin_memory=True, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Run model training benchmark with proper evaluation and test.\n",
    "    Returns training metrics, data loading throughput, and model performance.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSetting up training benchmark...\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Num workers: {num_workers}\")\n",
    "    print(f\"  Prefetch factor: {prefetch_factor}\")\n",
    "    print(f\"  Pin memory: {pin_memory}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    # Define transforms - aligned with train.py\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ImageFolder(os.path.join(data_dir, 'training'), transform=train_transform)\n",
    "    val_dataset = ImageFolder(os.path.join(data_dir, 'validation'), transform=val_test_transform)\n",
    "    eval_dataset = ImageFolder(os.path.join(data_dir, 'evaluation'), transform=val_test_transform)\n",
    "    \n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"  Training: {len(train_dataset)} images\")\n",
    "    print(f\"  Validation: {len(val_dataset)} images\")\n",
    "    print(f\"  Evaluation (test): {len(eval_dataset)} images\")\n",
    "    print(f\"  Classes: {len(train_dataset.classes)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    # Create MobileNetV2 model - aligned with train.py architecture\n",
    "    model = models.mobilenet_v2(weights='MobileNet_V2_Weights.DEFAULT')\n",
    "    \n",
    "    # Modify classifier for 11 classes (same as train.py)\n",
    "    num_ftrs = model.last_channel\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_ftrs, 11)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # For benchmark, freeze backbone features (similar to train.py approach)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Training setup - aligned with train.py\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)  # 1e-4 as in train.py\n",
    "    \n",
    "    # Training metrics tracking\n",
    "    training_start_time = time.perf_counter()\n",
    "    epoch_times = []\n",
    "    data_loading_times = []\n",
    "    batch_processing_times = []\n",
    "    training_losses = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"\\nStarting training...\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.perf_counter()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        epoch_data_time = 0.0\n",
    "        epoch_compute_time = 0.0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            data_start = time.perf_counter()\n",
    "            \n",
    "            # Move to device\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            if use_gpu:\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            data_time = time.perf_counter() - data_start\n",
    "            epoch_data_time += data_time\n",
    "            \n",
    "            # Training step\n",
    "            compute_start = time.perf_counter()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if use_gpu:\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            compute_time = time.perf_counter() - compute_start\n",
    "            epoch_compute_time += compute_time\n",
    "            \n",
    "            # Statistics - following train.py pattern\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Progress update every 50 batches\n",
    "            if (i + 1) % 50 == 0:\n",
    "                avg_loss = running_loss / (i + 1)\n",
    "                train_acc = 100. * correct / total\n",
    "                data_throughput = total / (epoch_data_time + 1e-6)\n",
    "                print(f\"  Batch {i + 1:4d}: Loss={avg_loss:.4f}, \"\n",
    "                      f\"Train Acc={train_acc:.2f}%, \"\n",
    "                      f\"Data throughput={data_throughput:.1f} samples/sec\")\n",
    "        \n",
    "        epoch_time = time.perf_counter() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "        data_loading_times.append(epoch_data_time)\n",
    "        batch_processing_times.append(epoch_compute_time)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        training_losses.append(train_loss)\n",
    "        \n",
    "        # Validation - following train.py validation function pattern\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"  Epoch {epoch + 1} Results:\")\n",
    "        print(f\"    Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"    Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        print(f\"    Epoch Time: {epoch_time:.2f}s\")\n",
    "        print(f\"    Data Loading Time: {epoch_data_time:.2f}s ({100*epoch_data_time/epoch_time:.1f}%)\")\n",
    "        print(f\"    Compute Time: {epoch_compute_time:.2f}s ({100*epoch_compute_time/epoch_time:.1f}%)\")\n",
    "        \n",
    "        # Save best model (following train.py pattern)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"    Validation loss improved. Best model updated.\")\n",
    "    \n",
    "    total_training_time = time.perf_counter() - training_start_time\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    print(f\"\\nFinal evaluation on test set...\")\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    test_start_time = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in eval_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_time = time.perf_counter() - test_start_time\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_loss = test_running_loss / len(eval_loader)\n",
    "    test_accuracy = 100. * test_correct / test_total\n",
    "    \n",
    "    # Additional metrics using sklearn\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        # Training metrics\n",
    "        'total_training_time_sec': total_training_time,\n",
    "        'epochs': epochs,\n",
    "        'final_training_loss': training_losses[-1],\n",
    "        'best_val_accuracy': max(validation_accuracies),\n",
    "        'final_val_accuracy': validation_accuracies[-1],\n",
    "        'best_val_loss': best_val_loss,\n",
    "        \n",
    "        # Test metrics\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': precision * 100,\n",
    "        'test_recall': recall * 100,\n",
    "        'test_f1': f1 * 100,\n",
    "        'test_inference_time_sec': test_time,\n",
    "        \n",
    "        # Data loading performance\n",
    "        'avg_epoch_time_sec': np.mean(epoch_times),\n",
    "        'avg_data_loading_time_sec': np.mean(data_loading_times),\n",
    "        'avg_compute_time_sec': np.mean(batch_processing_times),\n",
    "        'data_loading_ratio': np.mean(data_loading_times) / np.mean(epoch_times),\n",
    "        'samples_per_sec_training': len(train_dataset) * epochs / total_training_time,\n",
    "        'total_samples_processed': len(train_dataset) * epochs,\n",
    "        \n",
    "        # System metrics\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': num_workers,\n",
    "        'total_train_samples': len(train_dataset),\n",
    "        'total_val_samples': len(val_dataset),\n",
    "        'total_test_samples': len(eval_dataset),\n",
    "        'trainable_parameters': trainable_params,\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, train_loader, val_loader, eval_loader\n",
    "    del train_dataset, val_dataset, eval_dataset\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training benchmark\n",
    "results = run_training_benchmark(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=DATALOADER_OPTIONS['batch_size'],\n",
    "    num_workers=DATALOADER_OPTIONS['num_workers'],\n",
    "    epochs=3,  # Short benchmark run\n",
    "    prefetch_factor=DATALOADER_OPTIONS['prefetch_factor'],\n",
    "    pin_memory=DATALOADER_OPTIONS['pin_memory'],\n",
    "    use_gpu=True if device == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  Total training time: {results['total_training_time_sec']:.2f}s\")\n",
    "print(f\"  Average epoch time: {results['avg_epoch_time_sec']:.2f}s\")\n",
    "print(f\"  Data loading ratio: {results['data_loading_ratio']:.1%}\")\n",
    "print(f\"  Training throughput: {results['samples_per_sec_training']:.1f} samples/sec\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Best validation accuracy: {results['best_val_accuracy']:.2f}%\")\n",
    "print(f\"  Final test accuracy: {results['test_accuracy']:.2f}%\")\n",
    "print(f\"  Test F1 score: {results['test_f1']:.2f}%\")\n",
    "\n",
    "print(f\"\\nSystem Configuration:\")\n",
    "print(f\"  Batch size: {results['batch_size']}\")\n",
    "print(f\"  Number of workers: {results['num_workers']}\")\n",
    "print(f\"  Trainable parameters: {results['trainable_parameters']:,}\")\n",
    "\n",
    "# Save detailed results\n",
    "results['timestamp'] = datetime.now().isoformat()\n",
    "results['run_id'] = RUN_ID\n",
    "results['rclone_config'] = RCLONE_OPTIONS\n",
    "\n",
    "print(f\"\\nResults saved with Run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3972697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for analysis\n",
    "results_file = RESULTS_CSV\n",
    "file_exists = os.path.exists(results_file)\n",
    "\n",
    "with open(results_file, 'a', newline='') as csvfile:\n",
    "    fieldnames = [\n",
    "        'timestamp', 'run_id', 'total_training_time_sec', 'epochs', \n",
    "        'final_training_loss', 'best_val_accuracy', 'final_val_accuracy', 'best_val_loss',\n",
    "        'test_loss', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_inference_time_sec',\n",
    "        'avg_epoch_time_sec', 'avg_data_loading_time_sec', 'avg_compute_time_sec', \n",
    "        'data_loading_ratio', 'samples_per_sec_training', 'total_samples_processed',\n",
    "        'batch_size', 'num_workers', 'total_train_samples', 'total_val_samples', 'total_test_samples', \n",
    "        'trainable_parameters', 'vfs_cache_mode', 'vfs_cache_max_size', 'vfs_read_chunk_size', \n",
    "        'buffer_size', 'transfers'\n",
    "    ]\n",
    "    \n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    \n",
    "    # Prepare row data\n",
    "    row_data = {\n",
    "        'timestamp': results['timestamp'],\n",
    "        'run_id': results['run_id'],\n",
    "        'total_training_time_sec': results['total_training_time_sec'],\n",
    "        'epochs': results['epochs'],\n",
    "        'final_training_loss': results['final_training_loss'],\n",
    "        'best_val_accuracy': results['best_val_accuracy'],\n",
    "        'final_val_accuracy': results['final_val_accuracy'],\n",
    "        'best_val_loss': results['best_val_loss'],\n",
    "        'test_loss': results['test_loss'],\n",
    "        'test_accuracy': results['test_accuracy'],\n",
    "        'test_precision': results['test_precision'],\n",
    "        'test_recall': results['test_recall'],\n",
    "        'test_f1': results['test_f1'],\n",
    "        'test_inference_time_sec': results['test_inference_time_sec'],\n",
    "        'avg_epoch_time_sec': results['avg_epoch_time_sec'],\n",
    "        'avg_data_loading_time_sec': results['avg_data_loading_time_sec'],\n",
    "        'avg_compute_time_sec': results['avg_compute_time_sec'],\n",
    "        'data_loading_ratio': results['data_loading_ratio'],\n",
    "        'samples_per_sec_training': results['samples_per_sec_training'],\n",
    "        'total_samples_processed': results['total_samples_processed'],\n",
    "        'batch_size': results['batch_size'],\n",
    "        'num_workers': results['num_workers'],\n",
    "        'total_train_samples': results['total_train_samples'],\n",
    "        'total_val_samples': results['total_val_samples'],\n",
    "        'total_test_samples': results['total_test_samples'],\n",
    "        'trainable_parameters': results['trainable_parameters'],\n",
    "        'vfs_cache_mode': RCLONE_OPTIONS['vfs_cache_mode'],\n",
    "        'vfs_cache_max_size': RCLONE_OPTIONS['vfs_cache_max_size'],\n",
    "        'vfs_read_chunk_size': RCLONE_OPTIONS['vfs_read_chunk_size'],\n",
    "        'buffer_size': RCLONE_OPTIONS['buffer_size'],\n",
    "        'transfers': RCLONE_OPTIONS['transfers'],\n",
    "    }\n",
    "    \n",
    "    writer.writerow(row_data)\n",
    "\n",
    "print(f\"Results appended to {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db658c3",
   "metadata": {},
   "source": [
    "## Part 6: Cleanup\n",
    "\n",
    "Run this cell to unmount and prepare for the next configuration test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmount the object store\n",
    "print(f\"Unmounting {MOUNT_POINT}...\")\n",
    "result = subprocess.run(f\"fusermount -u {MOUNT_POINT}\", shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"SUCCESS: Unmounted successfully!\")\n",
    "else:\n",
    "    print(f\"Warning: {result.stderr}\")\n",
    "\n",
    "# Clear rclone cache (optional - uncomment if you want to clear cache between runs)\n",
    "# subprocess.run(\"rm -rf ~/.cache/rclone/*\", shell=True)\n",
    "# print(\"Cleared rclone cache\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for next configuration!\")\n",
    "print(\"Go back to Part 2 and change the configuration, then re-run.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a489c",
   "metadata": {},
   "source": [
    "## Data Processing and Merge\n",
    "\n",
    "This section reads the benchmark CSV and GPU CSV files, cleans the data by removing zero GPU utilization entries, calculates GPU metrics per run, and creates a final merged CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b89ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "FINAL_CSV = 'final_benchmark_results.csv'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PROCESSING AND MERGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read benchmark results\n",
    "if os.path.exists(RESULTS_CSV):\n",
    "    df_benchmark = pd.read_csv(RESULTS_CSV)\n",
    "    print(f\"Loaded benchmark data: {len(df_benchmark)} runs\")\n",
    "    print(f\"Columns: {list(df_benchmark.columns)}\")\n",
    "else:\n",
    "    print(f\"ERROR: {RESULTS_CSV} not found!\")\n",
    "    df_benchmark = pd.DataFrame()\n",
    "\n",
    "# Read GPU utilization data  \n",
    "if os.path.exists(GPU_CSV):\n",
    "    df_gpu = pd.read_csv(GPU_CSV)\n",
    "    print(f\"Loaded GPU data: {len(df_gpu)} samples\")\n",
    "    print(f\"Columns: {list(df_gpu.columns)}\")\n",
    "else:\n",
    "    print(f\"WARNING: {GPU_CSV} not found - will proceed with benchmark data only\")\n",
    "    df_gpu = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean GPU data by removing zero utilization entries\n",
    "if not df_gpu.empty:\n",
    "    print(f\"\\nCleaning GPU data...\")\n",
    "    print(f\"Original GPU samples: {len(df_gpu)}\")\n",
    "    \n",
    "    # Remove entries with 0% GPU utilization\n",
    "    if 'gpu_util_percent' in df_gpu.columns:\n",
    "        df_gpu_clean = df_gpu[df_gpu['gpu_util_percent'] > 0].copy()\n",
    "        print(f\"After removing 0% utilization: {len(df_gpu_clean)} samples\")\n",
    "        removed = len(df_gpu) - len(df_gpu_clean)\n",
    "        print(f\"Removed {removed} zero-utilization samples ({removed/len(df_gpu)*100:.1f}%)\")\n",
    "        \n",
    "        if len(df_gpu_clean) == 0:\n",
    "            print(\"WARNING: No valid GPU data after cleaning!\")\n",
    "            df_gpu_clean = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"WARNING: No 'gpu_util_percent' column found in GPU data\")\n",
    "        df_gpu_clean = df_gpu.copy()\n",
    "else:\n",
    "    print(\"No GPU data to clean\")\n",
    "    df_gpu_clean = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92036f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate GPU metrics per run_id\n",
    "if not df_gpu_clean.empty and 'run_id' in df_gpu_clean.columns:\n",
    "    print(f\"\\nCalculating GPU metrics per run...\")\n",
    "    \n",
    "    # Group by run_id and calculate aggregated metrics\n",
    "    gpu_metrics = df_gpu_clean.groupby('run_id').agg({\n",
    "        'gpu_util_percent': ['mean', 'max', 'std', 'min'],\n",
    "        'mem_used_mb': ['mean', 'max'],\n",
    "        'mem_total_mb': 'first',  # Should be constant\n",
    "        'temperature_c': ['mean', 'max'] if 'temperature_c' in df_gpu_clean.columns else ['mean', 'max'],\n",
    "        'timestamp': ['first', 'last', 'count']  # For tracking duration and sample count\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    gpu_metrics.columns = [f\"gpu_{col[0]}_{col[1]}\" if col[1] != 'first' else f\"gpu_{col[0]}\" \n",
    "                          for col in gpu_metrics.columns]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    gpu_metrics = gpu_metrics.rename(columns={\n",
    "        'gpu_gpu_util_percent_mean': 'avg_gpu_utilization',\n",
    "        'gpu_gpu_util_percent_max': 'max_gpu_utilization', \n",
    "        'gpu_gpu_util_percent_std': 'gpu_utilization_std',\n",
    "        'gpu_gpu_util_percent_min': 'min_gpu_utilization',\n",
    "        'gpu_mem_used_mb_mean': 'avg_memory_used_mb',\n",
    "        'gpu_mem_used_mb_max': 'max_memory_used_mb',\n",
    "        'gpu_mem_total_mb': 'total_memory_mb',\n",
    "        'gpu_timestamp_count': 'gpu_sample_count'\n",
    "    })\n",
    "    \n",
    "    # Calculate memory usage percentage\n",
    "    gpu_metrics['avg_memory_usage_percent'] = (gpu_metrics['avg_memory_used_mb'] / gpu_metrics['total_memory_mb'] * 100).round(2)\n",
    "    gpu_metrics['max_memory_usage_percent'] = (gpu_metrics['max_memory_used_mb'] / gpu_metrics['total_memory_mb'] * 100).round(2)\n",
    "    \n",
    "    # Reset index to make run_id a column\n",
    "    gpu_metrics = gpu_metrics.reset_index()\n",
    "    \n",
    "    print(f\"Calculated GPU metrics for {len(gpu_metrics)} runs\")\n",
    "    print(f\"GPU metrics columns: {list(gpu_metrics.columns)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No GPU data available for metrics calculation\")\n",
    "    gpu_metrics = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge benchmark data with GPU metrics\n",
    "if not df_benchmark.empty:\n",
    "    if not gpu_metrics.empty:\n",
    "        print(f\"\\nMerging benchmark data with GPU metrics...\")\n",
    "        df_final = df_benchmark.merge(gpu_metrics, on='run_id', how='left')\n",
    "        print(f\"Merged data: {len(df_final)} runs\")\n",
    "        \n",
    "        # Fill missing GPU values with NaN (for runs without GPU data)\n",
    "        gpu_columns = [col for col in gpu_metrics.columns if col != 'run_id']\n",
    "        missing_gpu_runs = df_final[gpu_columns[0]].isna().sum()\n",
    "        if missing_gpu_runs > 0:\n",
    "            print(f\"Warning: {missing_gpu_runs} runs have no GPU data\")\n",
    "    else:\n",
    "        print(\"No GPU metrics to merge - using benchmark data only\")\n",
    "        df_final = df_benchmark.copy()\n",
    "        \n",
    "    print(f\"Final dataset columns: {len(df_final.columns)}\")\n",
    "    print(f\"Final dataset shape: {df_final.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No benchmark data available!\")\n",
    "    df_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final merged dataset\n",
    "if not df_final.empty:\n",
    "    print(f\"\\nSaving final dataset to {FINAL_CSV}...\")\n",
    "    df_final.to_csv(FINAL_CSV, index=False)\n",
    "    print(f\"SUCCESS: Saved {len(df_final)} runs to {FINAL_CSV}\")\n",
    "    \n",
    "    # Display summary of the final dataset\n",
    "    print(f\"\\nFinal dataset summary:\")\n",
    "    print(f\"  Rows: {len(df_final)}\")\n",
    "    print(f\"  Columns: {len(df_final.columns)}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nFirst 3 rows of final dataset:\")\n",
    "    display_cols = ['run_id', 'timestamp', 'total_training_time_sec', 'test_accuracy']\n",
    "    if 'avg_gpu_utilization' in df_final.columns:\n",
    "        display_cols.extend(['avg_gpu_utilization', 'max_gpu_utilization'])\n",
    "    if 'avg_memory_usage_percent' in df_final.columns:\n",
    "        display_cols.append('avg_memory_usage_percent')\n",
    "    \n",
    "    print(df_final[display_cols].head(3).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nData processing completed successfully!\")\n",
    "    print(f\"Final file: {FINAL_CSV}\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No data to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information for reference\n",
    "if not df_final.empty:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FINAL DATASET COLUMN REFERENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nBenchmark columns:\")\n",
    "    benchmark_cols = [col for col in df_final.columns if not col.startswith(('avg_gpu', 'max_gpu', 'min_gpu', 'gpu_', 'total_memory', 'avg_memory', 'max_memory'))]\n",
    "    for col in benchmark_cols:\n",
    "        print(f\"  {col}\")\n",
    "    \n",
    "    if 'avg_gpu_utilization' in df_final.columns:\n",
    "        print(\"\\nGPU metrics columns:\")\n",
    "        gpu_cols = [col for col in df_final.columns if col.startswith(('avg_gpu', 'max_gpu', 'min_gpu', 'gpu_', 'total_memory', 'avg_memory', 'max_memory'))]\n",
    "        for col in gpu_cols:\n",
    "            print(f\"  {col}\")\n",
    "    \n",
    "    print(f\"\\nTotal columns: {len(df_final.columns)}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
